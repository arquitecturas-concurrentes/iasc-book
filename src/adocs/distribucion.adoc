= Distribución

== Introduccion

Antes de hablar de distribución, es a veces ideal tratar de explicarlo de cómo se llega a ella. En general cuando se plantea la arquitectura de una aplicación, en cuanto a la arquitectura, en donde queda afuera lo que es el diseño e implementación del mismo, en general se plantea o se despliega por primera vez como un monolito, es decir, solo implementar todo nuestro sistema en un solo servidor. Ya sea un prototipo, o una versión temprana (beta) de nuestra aplicación, en general se plantea el monolito como arquitectura inicial, por costos y tiempo en el que se despliega la aplicación, y también porque no tenemos problemas que nos puede causar un sistema distribuido.

[.center.iasc-image]
image::distribucion/image2.png[,500]

Esta es una imagen de una arquitectura monolitica tradicional, con una aplicacion y una base SQL, que en general esta desplegada en un solo servidor. Algunos problemas que puede generar esta arquitectura es la siguiente:

* Incremento de la complejidad de la aplicacion, a medida que pasa el tiempo y la misma empieza a tener gran cantidad de componentes o va creciendo los requerimientos del sistema
* Cuando empieza a ser usado cada vez por más usuarios, esto hace que aumente la carga y que un solo servidor pueda no ser suficiente o incluso generar problemas con la carga que generan estos usuarios

Ante el aumento de la carga de usuarios, tráfico y mayor interacción entre la aplicación y la base de datos, esto genera una carga mayor en el único servidor que tenemos, por lo cual deberemos escalar. Hay dos tipos de escala:

=== Escala Vertical (Scale Up)

[.center.iasc-image]
image::distribucion/image6.png[]

Esta manera de escalar, también conocido como escalar para arriba o verticalmente (?) (scale up), es incrementando los recursos de un servidor o un nodo, o sea aumentar CPU, memoria, disco, etc.
Lo que nos permite este tipo de escala es que no necesitamos realizar cambios o desarrollo adicional en nuestro sistema, solo habrá un tiempo en el que nuestro sistema no estará disponible hasta una vez que hayan sido implementado estos cambios.
El problema de escalar verticalmente es que el costo empieza a ser exponencial en un momento, por lo que en un momento se puede hacer muy costoso un nodo único muy grande en donde cada vez las mejoras van a ser menores en cuanto a recursos.
También hay un momento en el que, si utilizamos servidores tradicionales y no mainframes, exista una limitación en cuanto a la tecnología, en general por limitaciones en cuanto a la CPUs

=== Escala Horizontal

[.center.iasc-image]
image::distribucion/image12.png[]

La otra manera de escalar, es horizontalmente también (Scale Out) es cuando en vez de incrementar los recursos, se agregan nuevos servidores con los mismos recursos. Lo bueno de poder escalar horizontalmente es que la aplicación no deja de estar disponible al escalar de esta manera. El costo con esta metodología es mucho más económica por este medio, que escalando verticalmente, otro aspecto interesante es que es más tolerante a fallos, ya que más copias de la aplicación están corriendo al mismo tiempo. Las grandes desventajas son que la arquitectura se complica, junto con el aumento del ancho de banda entre los distintos componentes que puede llegar a tener nuestro sistema una vez que hagamos scale out.

Con respecto a nuestra aplicación monolítica, podemos bien o replicar toda la aplicación con su bbdd, y que no tengan relación alguna cada una de las maquinas (imagen izquierda), o bien podemos solo replicar en cada uno de los servidores la aplicación y luego tener en otra instancia nuestra bbdd (imagen derecha).

[.center.iasc-image]
image::distribucion/image7.png[]

La motivación para la arquitectura de la imagen de la izquierda puede ser que se quiera distribuir por países el sistema, o sea, que en cada país que exista esta aplicación se dirija a un servidor que contiene todo su estado. La otra manera de escalar horizontalmente nuestro monolito nos permite que no tengamos tres bases de datos distintas, y que incluso el sistema pueda ser visto aun por el usuario como uno solo, o sea generalmente mediante alguna diferencia de endpoints, pero... como manejar la carga de manera que no se sobrecargue un solo servidor y los otros no?  Con un load balancer...

[.center.iasc-image]
image::distribucion/image13.png[]

Un load balancer nos va a permitir usar alguna estrategia por la cual la carga sea distribuida de manera equitativa. Esto puede ser por un algoritmo simple, tal como puede ser un round robin, o puede tener algún feedback de los nodos o servidores, que pueden dar al load balancer un poco más de información sobre a dónde redirigir el tráfico.

Pero esta es la única manera de escalar?? La respuesta es no.

Se puede escalar horizontalmente de varias maneras, existe un libro que es el de "`Art of Scalability`" de Michael Fisher que nos propone un interesante modelo de cubo de como escalar.

[.center.iasc-image]
image::distribucion/image9.png[]

En esta representación el punto de origen concuerda con el sistema monolítico en una sola instancia, y la arista de arriba a la derecha del cubo, nos representa la máxima escalabilidad de la aplicación. Y como el cubo es en tres dimensiones, eso nos dice que no solo hay una manera de escalar nuestra aplicación sino que hay varias que incluso podríamos llegar a combinar, ahora veremos cada una de las dimensiones

=== Eje X - Clonacion

Esta manera de escalar, es la que vimos al final de la sección anterior, es cuando tenemos varias réplicas de nuestra aplicación monolítica y que pueden o bien acceder a una bbdd. Otros temas a mencionar que no se hayan hecho antes es el tema de que al existir varias copias de nuestro sistema, tendremos que de alguna manera garantizar que los datos se mantengan consistentes, ya que varias copias van a estar accediendo a los mismos datos, por lo que tendremos que bien o implementar un sistema de lockeo de los datos o usar algo que nos garantice esto, que bien puede ser la base de datos.
Este esquema no nos resuelve aún el problema de una aplicación de alta complejidad, o sea.. Que aun nuestro código va a estar aun organizado como una aplicación monolítica, esto puede no ser una desventaja si no tenemos un basecode demasiado grande pero si lo fuese, por temas de complejidad o incluso de uso de servicios de terceros, tal vez sea un problema a nivel de desarrollo o mantenimiento.

=== Eje Y - Descomposición Funcional

En vez de tener varias instancias o nodos del mismo sistema, cada uno conteniendo todo el código del sistema, en este eje se va a descomponer nuestra aplicación entre los distintos nodos, esto es lo que se conoce generalmente como microservicios, y cada uno de estos va a ser responsable de una función.

[.center.iasc-image]
image::distribucion/image8.png[]

Y como podemos descomponer a nuestra aplicación?

* Podemos descomponer por medio de la función técnica de lo que hace ya sea un endpoint, o una funcionalidad, por ejemplo, se puede descomponer una aplicación en servicios de capacidad (capability-services) como login, usuarios, productos, etc. Abajo hay un pequeño ejemplo de un servicio de discusión, estilo reddit.
* Otra manera de descomponer es dividir por dominios a la aplicación, o sea por los recursos pero como un todo, y no por las funcionalidades, o sea si tenemos una aplicación de venta, podemos dividir la aplicación en: servicio al cliente, ventas, catalogo para el usuario. Esta división es más bien por entidades (Entity-Services)

También pueden combinarse esta división o usarse otro tipo de división del sistema, el esquema no es estricto en este sentido, por lo que nos da la libertad de escalar nuestra aplicación o más bien de dividirla de acuerdo a nuestro dominio. No hay ya necesidad de clonar toda la aplicación de esta manera. El problema es que esto genera otras desventajas y son que ahora nuestros microservicios además de trabajar con la bbdd a veces tendrán que coordinarse los unos a los otros mediante mensajes o requests incluso, también puede haber un incremento en el uso del ancho de banda.

=== Eje Z - Sharding/Data Partitioning

Hasta ahora vimos cómo dividir nuestro sistema pero a nivel de organización de la lógica de nuestros componentes, puede existir el caso en el que si estamos persistiendo los datos de nuestro sistema, el mismo aún tenga una sola base de datos o a lo sumo este replicada en otras bases, pero que en suma estas réplicas son solo una copia de todo el estado de la base de datos principal, por lo que en ese esquema solo una base de datos admite escrituras y todas puedan ser leídas. Dependiendo del tipo de réplica, la misma puede ser hecha ni bien se escriben los valores o a veces pueden tenerse escrituras que no garantice que una vez que se escribió el valor en la base de datos, el mismo esté replicado en las otras bases, esto se llama replicación asincrónica. Pueden ver mas de esto en el siguiente (link)[https://www.cybertec-postgresql.com/en/services/postgresql-replication/synchronous-synchronous-replication/]

Pero aún tenemos el estado, a lo sumo replicado, tal como lo teníamos con los servidores al escalarlo mediante clonación (eje x), y solo podemos tener una escritura a la vez, esto puede ser un cuello de botella si existen muchas escrituras sobre la base de datos. El esquema de este eje z, es el de dividir la base de datos o el estado en subsets, por lo que podemos dividir el estado bajo un criterio determinado, por ejemplo si tenemos un estado que son los productos, bien podemos dividir los mismos por rango de letras como el siguiente ejemplo.

[.center.iasc-image]
image::distribucion/image4.png[]

Y como tenemos ahora el estado particionado, vamos a tener que tener algo que nos pueda despachar al nodo correcto los pedidos de lectura o escritura, que puede ser un load balancer, o bien que los nodos de estado o persistencia se puedan hablar entre ellos para devolver o persistir el estado, esto último es en el caso en el que no se tenga una base de datos relacional o tan solo se pueda implementar un shardeo de este tipo.
Lo interesante de este esquema es que no tenemos más de un nodo de datos que pueda aceptar escrituras, aunque solo habrá una escritura por subset.

== Combinación de Ejes

Como el modelo de escala es tridimensional, se puede incluso combinar para que nuestra arquitectura sea más robusta, por ejemplo, podemos tener un esquema de microservicios en el que además cada servicio, como el de login, puede esta clonado y tener más de un servicio de login disponible y el mismo esté tan solo distribuido por un segundo nivel de load balancer que redirige los request con un criterio, que nos da un esquema así? Como dijimos antes, mayor robustez, si bien escalar por microservicios divide la carga, aun todos los usuarios en gran medida, deberán loguearse, y ademas al separar los servicios, aun seguimos teniendo un solo punto de fallo, por lo que para que el servicio esté disponible si el servidor de login se cae, es el de escalar después este microservicio por medio de clonar el mismo en varios servidores idénticos con este microservicios.

[.center.iasc-image]
image::distribucion/image5.png[]

=== Sesiones

Otro tema que no hablamos antes es sobre las sesiones en nuestra aplicación ahora distribuida, en el primer caso que es el de replicar la aplicación, si tenemos varios nodos con toda nuestra aplicación y un load balancer, puede ocurrir que un usuario se loguee en un nodo y luego en otro request, pueda ser redirigido a otro nodo donde no se tiene registro de su login, como se resuelve esto? O bien teniendo un lugar, como una base de datos aparte donde se registren las sesiones o bien se puede optar por un esquema de sticky sessions

[.center.iasc-image]
image::distribucion/image10.png[]

ELs sticky sessions permiten mediante una cookie, saber contra qué servidor se autenticaron, además de otra información de la sesión, el load balancer sabe a qué nodo redirigir los requests de un usuario en particular sobre un modelo de cloning o en el que nuestra aplicación esta solo replicada, en otros tipo de arquitecturas como microservicios, puede ser que no sea necesario a menos que exista solo nodo de login de usuarios.

=== Heartbeat y gossip

A veces es necesario saber si un nodo o servicio esta disponible y que sepamos que no este caido, a veces si es solo un servicio que estamos exponiendo podemos tan solo tener un healthcheck, que es solo un endpoint de nuestro sistema (algo mas de detalle en https://microservices.io/patterns/observability/health-check-api.html, puede ser incluso implementado si no estamos en un esquema de microservicios) para confirmar si nuestro servicio esta disponible y puede admitir nuevos pedidos, pero esto es en general a nivel de HHTP, por lo que si un nodo nuestro se cae podriamos no llegar a poder responder un request hecho a nuestro endpoint.
Entonces cómo podemos detectar fallas en nodos de nuestro sistema si no es solo mediante un chequeo de healthcheck o un mecanismo similar? Podemos hacerlo mediante conexiones de tcp en las que se pase un token o un bit o un valor que se van a ir pasando entre los nodos, y de esta manera cuando un nodo deje de pasar ese valor después de un tiempo prolongado, muy superior al tiempo entre emisión de este valor, se puede tomar al nodo como caido.
Esto se lo conoce como heartbeat, y es útil para conocer si se cayeron los nodos sin depender de un healthcheck, ya que pudo existir una caída del nodo y que no sea un problema lógico o de algún componente o servicio externo. Lo recomendable es que la información de heartbeat este en una vía o conexión distinta a lo que se usaría para intercambiar información entre nodos o servidores.
Otra manera un poco mas bien ordenada o distribuida, de cómo notificar a otros servidores y no hacer múltiples conexiones entre nodos, para usar heartbeat, es el de usar un protocolo de gossip. Esto permite que se intercambie más información entre servidores, y que no sea solo un simple valor o bit, por ejemplo, puede ser información tal como por ejemplo: "`se cayó el nodo de login x`", o "`volvió a estar disponible el nodo que contiene la base de datos`". Se puede encontrar un poco más de información en el siguiente https://github.com/gossiperl/gossiperl/wiki/gossip-in-computer-science[link]

Videos que nos parecieron interesantes

https://www.youtube.com/watch?v=PE4gwstWhmc[How We've Scaled Dropbox]

https://www.youtube.com/watch?v=Y6Ev8GIlbxc[Distributed Systems in One Lesson by Tim Berglund]
